\documentclass[12pt]{article}

\usepackage{sbc-template}
\usepackage{graphicx,url}
\usepackage[latin1,utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage{amssymb,amsmath}

%\usepackage[usenames,dvipsnames]{color}
\usepackage{verbatim}
\usepackage{csquotes}
\usepackage{float}

\usepackage[backend=biber,natbib]{biblatex}
\addbibresource{sbc-template.bib}
\usepackage{url}



\usepackage{listings}
\usepackage{color}
 
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
 
\lstset{
  language=Python,                
  basicstyle=\footnotesize,           
  numbers=left,                   
  numberstyle=\tiny\color{gray},  
  stepnumber=2,                             
  numbersep=5pt,                  
  backgroundcolor=\color{white},    
  showspaces=false,               
  showstringspaces=false,         
  showtabs=false,                 
  frame=single,                   
  rulecolor=\color{black},        
  tabsize=2,                      
  captionpos=b,                   
  breaklines=true,                
  breakatwhitespace=false,        
  title=\lstname,                               
  keywordstyle=\color{blue},          
  commentstyle=\color{dkgreen},       
  stringstyle=\color{mauve},     
}


\sloppy



\title{Artificial Intelligence for predicting the popularity of web content: state of the art, taxonomy and research challenges}

\author{Sidney Loyola de Sá, Aline Paes, Antonio A. de A. Rocha}

\address{Computing Institute (IC)\\
        Universidade Federal Fluminense (UFF)\\
  \email{sidney\_loyola@id.uff.br, \{arocha,alinepaes\}@ic.uff.br}}
  


\begin{document} 

\begin{titlepage}
\maketitle
\end{titlepage}

\begin{abstract}

With the development of web 2.0 and the ease of creating and disseminating online content, the amount of information available on the Web has increased considerably in recent decades. However, only a small part of this content is likely to become popular. In this way, knowing beforehand which content is going to attract attention on the Internet is an economically valuable asset. Besides, obtaining such information would allow network administrators to better allocate the available resources according to traffic demands. However, identifying which content is going to be popular or not is a complex task as the variables related to such task are unknown. Thus, this problem has been investigated as a prediction task of Machine Learning. Also, previous works have pointed out that linguistic attributes, obtained with Natural Language Processing methods, are beneficial for predicting popularity from the content itself. This short course proposes the presentation of these models and techniques of the Artificial Intelligence area that aim to predict the popularity of content on the Web before its publication. To this end, the taxonomies related to the problem, state-of-the-art to solve this task, and the research challenges are going to be presented and discussed in this short course.
    
\end{abstract} 



\section{Introduction}

The Internet is accessed by more and more people, making it one of the primary means of communication and information in the world. Besides, the growing number of social networks and platforms on the Web has facilitated and democratized producing and publishing online content \cite{tatar2014survey}. Thus, users who were previously mere consumers of information have become producers and distributors of content. That is, small distributors, compete with large media companies for the public's attention on the Web.

        
However, the facility that allows for large-scale content production and thousands of content producers has brought with it the disadvantage of Internet saturation. In front of an immense amount of information, we noted that few contents attract most users' attention. Consequently, in the digital world, understanding which online content will capture users' attention, whether for entertainment or business, has become a valuable asset. \cite{tatar2014survey}. 
        
To give an idea of the amount of information released today, every day, users around the world send more than 500,000 tweets \cite{Telegraph2019} and watch over a billion hours of videos, generating billions of views \cite{YouTube2019}. Despite all this amount of information, the online ecosystem adheres to the concept ``the winner-take-all", making few items retain most of the attention. In this context, identifying the web content that will be popular becomes relevant. \cite{tatar2014survey}. 
        
The ability to predict the popularity of content on the Web has several practical applications, both for content producers and for marketing and infrastructure companies, such as: maximizing the return on marketing investment, proactively allocating network resources, adjusting them accurately to future demands, select the best content for the audience, direct investments to the content to be produced, among other advantages.
    
Finding out in advance which content will be popular is not trivial. Several variables can directly influence the forecast, including the topics covered, the lexical content, the linguistic or visual style, the authors of the content, the target audience, other related publications, among many others.
        
Because of so many influencing factors, researchers developed several strategies to improve the forecast's performance. The application of Artificial Intelligence, primarily, Machine Learning and Natural Language Processing, has obtained good results in the task of predicting popularity, as seen in \cite{bandari_2012}, \cite{hutchison_predicting_2012},\cite{pereira_proactive_2015}, \cite{liu_predicting_2017} e \cite{khan_news_2018}.  
        
Machine Learning allows a model to learn, in conjunction with a previous data set, adapting to new circumstances \cite{russel_inteligencia_2013}. Natural Language Processing Techniques, among other possibilities, allow discovering features that contribute significantly to the popularity forecast \cite{pereira_proactive_2015},\cite{khan_news_2018}. 
        
In 2014, \citet{tatar2014survey} presented a study on the primary forecast surveys, specifying a classification focusing on the objective and timing of forecast execution: before published content or after publication. Recently, \citet{moniz_review_2019} prepared an excellent review of predictive models proposing a classification focused on three elements: objective, selection of predictive attributes, and methods of data mining/machine learning.

This research proposes a different approach from the previous reviews; we present a taxonomy that classifies the models through predictive attributes to be used. The models use the Supervised Learning approach and, whenever possible, Natural Language Processing to forecast video data and news articles available on the Web. We also include recent studies that obtain predictive attributes directly from the video frames \cite { trzcinski_predicting_2017},\cite {trzcinski_recurrent_2017}.
    
The rest of the work is divided as follows: the section \ref {fundamentacao} introduces basic concepts of Machine Learning and Natural Language Processing, the section \ref {previsao} presents the concepts of Popularity Prediction, its operation, types of content and taxonomy, in the \ref {metodos} section the main Classification methods found in the literature are presented, in the \ref {regressao} section the Regression methods are presented and, finally, in the section \ref {conclusao} the conclusions are presented, as well as the challenges and future work.

\section{Theoretical Foundation}\label{fundamentacao}

\subsection{Machine Learning}
    
Machine Learning (ML) is a subfield of Artificial Intelligence, and the motivation for its emergence was the idea of equipping machines with the ability to solve problems. Not every problem can be modeled and solved using a deterministic algorithm, which follows a step-by-step. For example, recognizing a person through his face, despite being a simple task for humans, is not trivial for a machine. The many variables involved make it difficult to implement. In front of these situations, Machine Learning algorithms build knowledge without being ``programmed'\ to do so, ``learning" \ through examples \cite {faceli_inteligencia_2011}.
            
This learning is, in fact, the search for a function capable of solving the problem to be addressed. Using previous inputs and outputs, the algorithms, by themselves, create rules capable of achieving a certain objective. This approximation of function (or hypothesis induction) using previous experiences is called Machine Learning (ML) \cite {faceli_inteligencia_2011}.
            
There are three paradigms in which learning tasks are commonly classified: supervised learning, unsupervised learning, and reinforcement learning.
    
\textbf {Supervised Learning}. In this paradigm, the tasks are predictive, and the training data set must have attributes (characteristics) of input and output (also called target). The exits must be labeled simulating the activity of a supervisor, that is, someone who knows the `` answer ''. The supervised learning task can be described as \citet {russel_inteligencia_2013}, as follows:
    
\begin{displayquote}
   Given a training set of N sample input and output pairs
   \[(x_1,y_1),(x_2,y_2),...(x_n,y_n),\]
    where each $y_i$ was generated by an unknown function $y=f(x)$, 
the algorithm must find a function $h$ that approximates the true function $f$.
\end{displayquote}
    
The hypothesis $ h $ function must be valid for other objects in the same domain that do not belong to the training set. This property is called generalization. The low capacity for generalization means that the data is over-adjusted (overfitting), that is, it is as if the model was not ``learning''\ and the reverse case indicates under-adjustment of the data (underfitting) \cite {faceli_inteligencia_2011}.
            
It is adopted as a good practice, when dealing with supervised learning, to use three sets of data: training, validation, and testing. The training set is used to adjust the model. That is, we provide the data for the algorithm to learn (find a function that approximates $ f $) from known examples. The validation set is important for assessing the model's generalizability, verifying that it is neither over-adjusted nor under-adjusted. Finally, with the test set, the performance of the model is assessed, verifying whether it solves the proposed problem or not.
     
Predictive tasks can be divided into problems of \textbf {classification}, when the output $ y $ wanted is a set of qualitative values, for example, the health status of a patient (\textit {healthy}, \ \textit { sick}), when the output is a numerical value, eg: \textit {temperature}, the task is called \textbf {regression}. \citet {faceli_inteligencia_2011} presents the following definitions:
            
\begin{itemize}
    \item Classification: $y_{i}$ = $f(x_{i}) \ \in \ \left\{ c_{1},...,c_{m} \right\}$, that is, $f(x_{i})$ accepts values in a discrete and unordered set;
    \item Regression: $y_{i} \ \in \ \mathbb{R}$, that is, $f(x_{i})$  accepts values in a infinite and ordered set;.
\end{itemize}
            
\textbf {Unsupervised Learning}. These tasks are descriptive, do not use the output attributes. That is, the data set does not need to be labeled. The objective is to find some order in the data, separating them into similar groups (grouping or clustering) or finding rules of association between groups of attributes \cite {faceli_inteligencia_2011}.

            
 \textbf {Reinforcement Learning}. These tasks simulate learning by trial and error, rewarding positive actions, and punishing negative actions. 

The following is a brief description of the main Machine Learning methods used to predict popularity:
            
    \begin{itemize}
        \item \textbf{k-NN}. The algorithm of the nearest neighbors (k-NN) is a distance-based method, so the input example will be classified according to the objects that are close to it in the training set. The k parameter defines the number of objects to be taken into account for the forecast. It is considered a lazy algorithm because it only memorizes the data of the training set, without necessarily learning a model. It has difficulty in dealing with a large number of attributes (dimensionality). The classification process is slow because it covers the entire data set \cite {faceli_inteligencia_2011}.
        \vspace{5mm} %5mm vertical space
        
        \item \textbf{\textit{Naive} Bayes}. The  Naive Bayes algorithm is a probabilistic model that uses Bayes' theorem for cases where the values of the attributes are independent of each other. Thus, the probability of an item belonging to a certain class is represented in the equation \ref {naive_bayes} and the algorithm associates the example $ x $ with the class $ y_ {k} $ for which $ P (y_ {k} | x) $ is maximum.
    
        \begin{flushright}
            \begin{equation}\label{naive_bayes}
                P(y_{i}|x) \ \propto \ P(y_{i}) \prod_{j=1}^d P(x_{j}|y_{i})
            \end{equation}
        \end{flushright}
    
        \item \textbf {Decision Tree}. The formal definition of a decision tree presented in \citet {faceli_inteligencia_2011} is: ``a decision tree is a directed acyclic graph in which each node is either a division node, with two or more successors or a leaf node". Each division node presents a test that makes the model decide the path to be taken. Leaf nodes are labeled with functions, representing rules and conclusions, which divide the space of entries into subspaces \cite {faceli_inteligencia_2011}. There are many algorithms that use the formality of decision trees, one of the best known being C4.5 \cite {quinlan_c45_1993}, which has a JAVA implementation with the name ``J48"\ \cite {araujo_vieira_avaliacao_2018}.
            
            \vspace{5mm} %5mm vertical space
            
       \item \textbf {Support Vector Machines - SVM}. They were developed using statistical learning theory looking for hyperplanes that linearly separate the data sets \cite {drucker_support_nodate}. ``The equation for a hyperplane is presented in the equation \ref{eq:hiperplano}, where $ w\ . \ x \ $ \ is the scalar product between the vectors $ w \ and \ x $, $ w \in X $ is the normal vector for the described hyperplane and $ \frac {b} {|| w ||} $ corresponds to the distance of the hyperplane in relation to the origin, with $ b \in \Re $ "\ \cite {faceli_inteligencia_2011}.
       
       \begin{flushright}
           \begin{equation}
           \label{eq:hiperplano}
               h(x) = w\ .\ x\ + \ b
           \end{equation}
       \end{flushright}
       
        \item \textbf {Artificial Neural Networks}. Neural networks were inspired by the model of biological synapses, trying to simulate the human brain. They are composed of simple processing units, artificial neurons, arranged in layers, and interconnected by a large number of connections. When all neurons in one layer are connected to the next layer, it is said to be a completely connected layer. ``The feedback connections allow a neuron to receive, at its input terminals, the output of a neuron from the same or a posterior layer" \ \cite {faceli_inteligencia_2011}.
        \vspace{5mm} %5mm vertical space
        
        The networks with backpropagation (feedback) are indicated to process sequential information, being successfully applied in natural language processing. The weights associated with the connections weigh the importance of the inputs, so the learning algorithms seek to find the values of these weights that optimize the output. The network architecture defines the number of neurons and the number of layers depending on the problem to be solved \cite {faceli_inteligencia_2011}.
            
            
            \vspace{5mm} %5mm vertical space
            
        \item \textbf {Perceptron}. It was the first neural network to be implemented, therefore, it was also the first neural network to be trained. It uses the neuron model presented in \citet {mcculloch_logical_1943} which uses the threshold type activation function, its architecture has only one layer with a single neuron. The weights are adjusted according to the equation \ref{perceptron}, where $ w_ {j} (t) $ is the weight of the \textit {j}-th input connection at time point $ t $, $ \eta $ is a learning rate, $ x_ {i} ^ {j} $ is the value of the \textit {j}-th attribute of the input vector $ x_ {i} $, $ \hat {f} (x_ {i} ) $ is the output produced by the network \cite {faceli_inteligencia_2011}.
        
            \begin{flushright}
                \begin{equation} \label{perceptron}
                    w_{j}(t+1)=w_{j}(t)+\eta x_{i}^j(y_{i}-\hat{f}(x_{i}))
                \end{equation}
            \end{flushright}
            
            
        Single-layer networks solve only linearly separable problems, but by adding layers and using non-linear activation functions in the intermediate layers, it is possible to solve non-linear problems. Multilayer networks are also called multilayer perceptron networks (MLP) \cite {faceli_inteligencia_2011}.
        
        \vspace{5mm} %5mm vertical space
        
        \item \textbf{Bagging}. The name of the method derives from bootstrap aggregating; the basic idea is to generate several predictors for the same data set, aggregating them to obtain a better result. Instead of choosing a single model for the forecast, we will use a result of several models. A numerical forecast used the predictors' average, while in the classification, we use the majority vote \Cite {breiman_bagging_1996}.
        
        \vspace{5mm} %5mm vertical space
        
        \item \textbf{AdaBoost}. Implementing a boosting algorithm that incrementally improves the accuracy of a predictor by adapting the parameters of the model. It combines several predictors to improve the result. However, this algorithm's main contribution is that it is unnecessary to know the accuracy of the `` weak '' predictors. The algorithm improves performance iteratively by generating the weights to be used by the models in the next step \cite {freund_decision-theoretic_1997}.
        
        \vspace{5mm} %5mm vertical space
        
        \item \textbf{Random Forest}. In this method, several decision trees are created to be used together in the forecast. The algorithm randomly chooses a subset of variables from the training set, and the decision of which variable will belong to the decision node is also made at random. This process repeats several times until it converges to the point where a new tree's construction does not improve performance. This method usually brings good results, but it demands computational power \cite {ho_random_1995}.
        
    \end{itemize}
    
\subsection{Natural Language Processing}
           
Natural Language Processing (NLP) is a subfield of Artificial Intelligence that investigates methods and techniques through which computational agents can communicate with humans. Besides, these techniques can extract information from the written language \cite {russel_inteligencia_2013}.
            
The Web records much of human knowledge through countless pages of information. Computational agents can ``learn" \  and extract information from this extensive database, but they need to deal with the natural language used by humans, which is ambiguous and confusing. This problem is divided into information search tasks in text classification, information retrieval, and information extraction. These tasks use language models, which are probabilistic distributions of expressions found in the texts \cite {russel_inteligencia_2013}.
            
Computers use formal languages, such as Java or Python programming languages. The sentences in these languages are defined by a syntax, allowing to define whether a set of strings is valid or not in a given language \cite {russel_inteligencia_2013}.
            
Unlike formal languages, human communication allows ambiguities that do not allow characterize Natural languages as a set of definitive sentences. Therefore, search the probability of a sentence belonging or not to a specific language  \cite {russel_inteligencia_2013}.
            
According to \citet {russel_inteligencia_2013} the probability distribution over string can be written $ P (c_ {1: n}) $, for example $ P (a) = 0.27 $ and $ P (zgq) = 0, $ 00000002. We call these probability distribution sets n-gram models. These models are defined as a \textbf {Markov chain} of order $ n-1 $, in these chains the probability of the character $ c_ {i} $ depends on the immediately preceding characters. In the equation \ref {3-grama} we have a trigram model (3-gram) \cite {russel_inteligencia_2013}.
            
            \begin{flushright}
                    \begin{equation} \label{3-grama}
                        P(c_{i}|c_{1:i-1})=P(c_{i}|c_{i-2:i-1})
                    \end{equation}
            \end{flushright}
            
The \textbf {corpus} is a body of text used to acquire the characteristics of the language, in this case, the trigrams' probabilities. In a 100-character language, a 10 million character \textbf {corpus} is required to obtain the probability distribution of a trigram model. The n-gram templates can be used for language recognition, spelling correction, gender classification, and recognition of named entities \cite {russel_inteligencia_2013}.
            
Another way to extract information from a text is using a discriminative model. In these models, the probability of hidden attributes is conditional depending on the observed text. Thus, given a text $ e_ {1: n} $, the model finds the hidden state sequence $ X_ {1: n} $ that maximizes $ P (X_ {1: n} | e_ {1: n}) $ . An example would be \textbf {conditional random fields} \cite {russel_inteligencia_2013}.
            
Stanford NER \footnote {https://nlp.stanford.edu/software/CRF-NER.html} is a JAVA implementation of a named entity recognizer. This software is already pre-trained to recognize people, organizations, and places in the English language. It uses linear field random field models incorporating non-local dependencies for information extraction, as presented in \citet {finkel_incorporating_2005}.
            
            
\section{Popularity Prediction} \label{previsao}
    
\citet {tatar2014survey} summarized the central studies on popularity forecasting reporting the initial approaches that tried to understand the pattern of distribution of users' access to different types of content \cite {crane}. After mapping these popularity probability distributions (mainly from videos), a search for models capable of predicting the popularity of information on the Web began \cite {tatar2014survey}.
        
It is interesting to note that the use of the popularity forecast in conjunction with other technologies such as: increasing the hit rate of cache relocation algorithms \cite {famaey_towards_2013}, optimization of news articles \cite {pereira_proactive_2015}, associated with systems recommendation \cite {tan_predicting_2019} among others.

One of the first works that present a predictive method with Machine Learning was \citet {szabo2010predicting}, showing a linear correlation between the different moments of the content life cycle on the Web. The predictive model presented was used in other studies, including the proposal on-demand video service cache reallocation  \cite {famaey_towards_2013}.
        
This section presents state of the art and the taxonomy built from the methods involved in Popularity Prediction. We present definitions, the operation of popularity forecasts, the types of content, and a Taxonomy to classify the models studied. 
        
\subsection{Definitions}
    
Below we present the main definitions used in this text and most studies on the topic.
\begin{itemize}
    \item \textbf{Web Content}.  It is defined as an individual item available on a website in text, audio, image, or video format \cite {tatar2014survey}. 
    \item \textbf {Popularity of content} is the relationship between an individual item and the users who consume it. Popularity is represented by a metric that defines the number of users attracted by the content being studied, reflecting the online community's interest in this item \cite {tatar2014survey}.
    \item \textbf {Data set}. They are formed by objects representing a physical object or an abstract notion \cite {faceli_inteligencia_2011}. In our case, it represents web content.
    \item \textbf {Feature}. Characteristic of the content, obtained directly or derived (through some calculation or technique). Each attribute is associated with an object property (web content) \cite {faceli_inteligencia_2011}.
    \item \textbf {Predictive attributes}. These are features used as inputs for machine learning models. Usually, the entry is represented by an attribute vector \cite {faceli_inteligencia_2011}.
    \item \textbf {Target attribute}. Also called target or exit, it represents the phenomenon of interest of the forecast, in our case the popularity \cite {faceli_inteligencia_2011}.
    
\end{itemize}
    
    
Looking at the ``most popular'' \ videos or texts on the web, the concept of popularity is intuitively understood. However, it is necessary to define objective metrics in terms of their popularity for comparing two items. Several measures signal which content attracts the most attention on the web; that is, the number of users willing to consume the item searched. The ``classic'' \ web metric is the number of views. It is not always available, and in some cases, does not represent the relationship of interest between the content and the users.
            
Therefore, the metric's definition to be used, which may vary according to the context, is essential in a study on popularity forecasting. In the literature review, the main metrics and their respective meanings are: number of views, reflects the number of users \cite {szabo2010predicting, trzcinski_recurrent_2017}, number of shares, reflects the notoriety of the content \cite {pereira_proactive_2015, khan_news_2018}, number of tweets and comments, reflects the time users spend on content \cite {bandari_2012}.
    
\subsection{Content Types}
    
The attention of users on the web is spread over several sites and various types of content, some of the most popular are: videos produced by users, responsible for much of the Internet traffic, news articles shared and consumed on mobile devices, stories published in news aggregators and items (comments, photos, videos) published on social networks \cite {tatar2014survey}.
            
The concepts presented can be applied to any type of content available online. However, to define the work scope, we will present methods and techniques that predict the popularity of videos, news articles, and images using Machine Learning and Natural Language Processing.
            
\textbf{Videos Online.} YouTube \footnote {www.youtube.com}, the largest online video platform uploading over 100 hours per minute \cite {YouTube2019} and over one trillion videos viewed per year, has been the main focus of studies \cite {tatar2014survey}. Studying the popularity of YouTube content is challenging due to the growing number of videos, the various attributes provided by the platform, and the limitations associated with selecting a representative subset of videos for the problem in question \cite {borghol}.
    
The number of views usually expresses the popularity of videos on the web. It follows a long-tail distribution but depending on the set of videos chosen. There may be other probability distributions, such as Power-law, log-normal, Weibull, or Gamma \cite {tatar2014survey}. A more detailed analysis reveals that, during periods of peak popularity, different videos' activities follow similar patterns \cite {crane}.
    
\textbf{News Articles.} The primary source of information in the digital world, news articles, are created and distributed massively through social networks. While videos attract users' attention over a long period, interest in the news is temporary, with their attention span a few days after publication. The popularity metric often used is comment numbers, as news platforms rarely disclose the number of views \cite {tatar2014survey}.
    
The different types of content have different characteristics, thus selecting those that will be provided to the Machine Learning models. As input attributes, it is a significant part of the popularity forecast. The choice of these attributes directly influences the quality of the predictive models. In addition to models' choice, it is essential to find a correlation between the predictive attributes and the content's popularity \cite {uddin_predicting_2016}. Several factors that influence this prediction are challenging to measure, such as quality of content, the relevance of the author, and users' relevance.
    
There are some apparent attributes to select and others, not so obvious, that strongly impact predictive models. Some influencing factors are already well established. For example, videos that evoke strong and positive emotions are among the most shared, in addition to being the ones that spread the most quickly \cite {berger_what_2011}. In this case, it is possible to perform a Sentiment Analysis to determine the content polarity \cite {pereira_proactive_2015}.
    
The different types of content have different characteristics; thus, Despite all efforts to find attributes that make items popular, the truth is that high-quality content is among the most viewed. However, quality is a difficult metric to measure, it involves subjective factors, so there is a great difficulty in determining attributes that capture the content's quality. Another factor, not trivial to include in predictive models, is the real world's events that directly influence which virtual content will be most sought after, impacting its popularity.
    
The most used predictive attributes are: characteristics of the content creators, for example, the authors with the highest audience tend to have popular content just for their identity \cite {bandari_2012}; and keywords that strongly impact popularity, both positively and negatively. In most studies, the categorization of content contributed positively to the number of views. Finally, attributes related to social networks such as the number of followers, online reputation, previous content that had many views, and a large number of shares also contribute to the increase in popularity \cite {pereira_proactive_2015}.

\subsection{Operation}

For the realization of the popularity forecast, we used the traditional machine learning flow. Despite that, studies do not report explicitly; most of them follow the model exemplified in \citet {khan_news_2018}, which has the following steps:

\begin{itemize}
    \item Data collection. Obtaining the necessary data is not always a trivial task. The central studies use the YouTube and Twitter platforms, which, despite not making the data openly available, there are already several APIs facilitating this collection. Both platforms have a defined metric and an ecosystem that allows determining the popularity of the published items.
    \item Pre-processing. In this step, we try to adjust the data for the use of the algorithms. In addition to searching for missing elements, we defined popularity classes, and the data set is balanced.
    \item Feature selection. This step is the center of current popularity forecasting research. The most recent work tries to automate this selection to try to get better and better results.
    \item Model Training. The forecast itself uses the models most relevant to the context of the problem to allow an efficient comparison with the research already carried out.
    \item Validation. In this stage, the performance of the models is tested and evaluated.
\end{itemize}

\subsection{Taxonomy}

To structure the study and presentation, we divided the methods of predicting popularity according to the problem definition and the prediction task, as follows:
    
\begin{itemize}
    \item \textbf {Regression Methods}. These methods perform a numerical forecast, quantifying the popularity according to the defined metric. The most common target attributes are the number of views, number of shares, number of tweets, and comments. These predictive methods use \textbf {Regression} and are often called regressors. \cite {szabo2010predicting, trzcinski_predicting_2017}.
    
     \ item \textbf {Classification Methods}. Popularity classes are defined; the predictive model allocates the content in one of the defined classes. The goal is to predict whether content will become popular or not; in most cases, only two classes are used: popular and non-popular. These predictive methods use \textbf {Classification} and are often called classifiers. \cite {pereira_proactive_2015, liu_predicting_2017, khan_news_2018, bandari_2012}.
\end{itemize}
    
In addition to the division, according to the task, we can group the forecasting methods, according to the attributes used:
    
\begin{itemize}
    \item \textbf {Textual Attributes}. These attributes are extracted from the item to have the expected popularity using natural language processing techniques. The extraction can be direct from the content. In news articles, it can be from the description presented on the web, as in videos and images, and even taking advantage of social media elements, such as comments published by users.
     \item \textbf {Visual Attributes}. These attributes are extracted from videos and images using Machine Learning techniques (Neural Networks, for example) or manually to select frames representative of the content.
     \item \textbf {Meta-Attributes}. These attributes are provided by the website where the content was published and inherent to the web. However, they do not belong to any previous groups, such as the source of the content, category, number of views, and publication date.
\end{itemize}
    
This taxonomy is shown in Figure \ref{fig:taxonomia}.
    
\begin{figure}[ht]
    \centering
    
\includegraphics[scale=0.5]{figuras/2_Taxonomia-Taxonomia_Design (1).png}
    \caption{Taxonomia dos Métodos de Previsão}
    \label{fig:taxonomia}
\end{figure}

\section{Classification Methods} \label{metodos}

In this section, we present the Machine Learning methods that use classifiers to predict web content's popularity. The definition of the class ``popular'' \ differs from one study to another, taking into account the data set used and the context of the research. Usually, popular content belongs to the minority class, causing the popularity classes to become unbalanced. , allowing the model to obtain \ ``high degree " \ of correctness, without necessarily, the algorithm classifies them correctly. For this reason, the definition of classes seeks a balanced distribution.
        
Performance evaluation and comparison between models is vital to find the best answer to the problem in question. Among the metrics used to evaluate the classifiers is \textbf {accuracy} defined by the equation \ref {acuracia}. This metric is the complement of \textbf {error rate}, or incorrect classifications, presented in the equation \ref {taxa_erro}. $ \hat {f} $ is the classifier, $ y_ {i} $ the known class of $ x_ {i} $ and $ \hat {f} (x_ {i}) $ the predicted class, $ I (a) = 1 $ if $ a $ is true and 0 otherwise. It is possible to present the error rate in a more understandable way as in the equation \ref{taxa_erro_simplificada} where FP are false positives and FN are false negatives \cite {faceli_inteligencia_2011}.
        
\begin{flushright}
    \begin{equation}\label{acuracia}
        ac(\hat{f})=1-err(\hat{f})
    \end{equation}
\end{flushright}

\begin{flushright}
    \begin{equation}\label{taxa_erro}
        err(\hat{f})=\frac{1}{n}\sum_{1=1}^n I(y_{i}\neq \hat{f}(x_{i}))
    \end{equation}
\end{flushright}

\begin{flushright}
    \begin{equation}\label{taxa_erro_simplificada}
        err(\hat{f})=\frac{FP+FN}{n}
    \end{equation}
\end{flushright}
        
        
Other metrics used are the \textbf {precision}, equation \ref {precisao}, which presents the `` proportion of positive examples correctly classified among all those predicted as positive" \ \cite {faceli_inteligencia_2011}, recall, equation \ref {recall}, which corresponds to the hit rate in the positive class. In the equations \ref {precisao} and \ref {recall}, VP is the number of true positives, FP are the false positives and FN is the number of false These equations were defined for models of two classes \cite {faceli_inteligencia_2011}.
        
\begin{flushright}
    \begin{equation}\label{precisao}
        prec(\hat{f})=\frac{VP}{VP+FP}
    \end{equation}
\end{flushright}

\begin{flushright}
    \begin{equation}\label{recall}
        rev(\hat{f})=\frac{VP}{VP+FN}
    \end{equation}
\end{flushright}
            
The precision presents an accuracy of the model, while the recall has completeness. Analyzing only the occurrence, it is not possible to know how many examples were not classified correctly. With the recall, it is not possible to find out how many examples were classified incorrectly. Thus, it is usually performed with the F-measure, which is the weighted harmonic mean of precision and recall. In the equation \ ref {measure-f}, $ w $ is the weight that weighs the importance of precision and recall. With weight 1, the degree of importance is the same for both metrics. The measure $ F_ {1} $ is presented in the equation \ref {medida-f1} \cite {faceli_inteligencia_2011}.
        
    \begin{flushright}
        \begin{equation}\label{medida-f}
            F_{m}(\hat{f})=\frac{(w+1) \times rev(\hat{f}) \times prec(\hat{f})}{rev(\hat{f})+ w \times prec(\hat{f})}
        \end{equation}
    \end{flushright}
    
    \begin{flushright}
        \begin{equation}\label{medida-f1}
            F_{1}(\hat{f})=\frac{2 \times rev(\hat{f}) \times prec(\hat{f})}{rev(\hat{f}) +  prec(\hat{f})}
        \end{equation}
    \end{flushright}
        
The ROC (Receiving Operating Characteristics) \ \cite {prati_curvas_2008} graph is represented in two dimensions ``with X and Y axis representing the measures of false positive rate (TFP) and true positive rate (TVP), respectively"\ \cite {faceli_inteligencia_2011}. In this graph, the diagonal represents a random classifier, so the best models can classify above this line, as shown in Figure \ref{fig:curva_roc}.
        
         \begin{figure}[ht]
                \centering
                
                \includegraphics[scale=0.5]{figuras/curva_roc.png}
                \caption{Exemplo de Curva ROC}
                \label{fig:curva_roc}
            \end{figure}
            
It is usual to construct a ROC curve to compare the performance between the different classification models, as seen in Figure \ref{fig:curva_roc}, and calculate the area under ROC curve (AUC). For the construction of the ROC curve, it is necessary to order the test cases according to the continuous value provided by the classifier (depending on the model, an adaptation may be necessary). With this ordered set there is ``a step of size $ \frac {1} {POS} $ in the direction of the y axis if the example is positive or a step of size $ \frac {1} {NEG} $ if the example is negative"\ \cite {prati_curvas_2008}.
        
        
\subsection{Textual Features}
        
Natural language processing techniques allow the extraction of several attributes directly from content, as in news articles, or from information provided, such as descriptions of videos and images. Among these techniques, there are the analysis of feelings, recognition of named entities, subjectivity of the text, and discovery of topics with the Latent Dirichlet Allocation (LDA) algorithm \cite {blei_latent_nodate}.
            
Twitter \footnote{https://twitter.com/}, one of the most popular social networks in the world, lets sharing information via short messages. News Articles are shared on Twitter by publishing the news URL and the retweet feature, which allows sending information without modification. In \cite {bandari_2012}, the authors used five classifiers with a set of multidimensional attributes to predict the popularity of news articles on Twitter through the number of \textit{tweets} and \textit{retweets}.
            
The news articles were collected from the news aggregator \textit {Feedzilla} \footnote {http://www.feedzilla.com/} and the attributes proposed in \cite {bandari_2012}, which tried to cover different dimensions of the problem, were:
           
    
            \begin{enumerate}
               \item The source of the news, which generated or published the article;
                 \item The category of the article, according to \textit {Feedzilla};
                 \item The subjectivity of the article's language;
                 \item Named entities present in the articles.
            \end{enumerate}
    
They collected data from August 8, 2011, to August 16, 2011, totaling 44,000 articles. For each article, the Topsy \footnote {http://topsy.com} tool provided the number of \textit {tweets}. To recognize named entities (places, people, or organizations), they used the Stanford-NER \footnote {http://nlp.stanford.edu/software/CRF-NER.shtml} tool.
            
For the articles' subjectivity, a Ling-Pipe classifier was used, which is a set of tools for processing natural language with Machine Learning algorithms developed in \citet {pang_sentimental_2004}. For training the tool, the authors used transcripts from Rush Limbaugh \footnote {http://www.rushlimbaugh.com} and Keith Olberman \footnote {http://www.msnbc.msn.com/ id / 32390086} as \textit {corpus} for subjective language. For the objective language training, the transcripts from the CSPAN \footnote {http://www.c-span.org} were used, as well as the transcription of articles from the website \textit {FirstMonday} \footnote {7http: // firstmonday .org}.
            
In the dataset collected, the absolute values do not reveal complete information. For example, the amount of news published by category does not emphasize the health category's importance. This category has few published articles, but they are among the most shared. For this reason, the measure \textit {t-density} was defined according to the equation \ref{t-density}. This measure was used to define a score for each category and another for the source of the article \cite{bandari_2012}.
            
            
            
                \begin{flushright}
                    \begin{equation} \label{t-density}
                        t-density = \frac{Number / of / Tweets}{Number / of  / links}
                    \end{equation}
                \end{flushright}
            
The dataset was divided into three classes, covering different ranges of popularity (\textit {tweets} and \textit {retweets} were counted as \textit {tweets}): Class A with up to 20 \textit {tweets}, Class B ranged from 20 to 100 and Class C with more than 100 \textit {tweets}, according to the table \ref{tab_classes}, taken from \citet {bandari_2012}. Articles that were not shared on Twitter, that is, with 0 \textit {tweets}, were not considered for the popularity forecast. For the classification, 4 machine learning methods were tested: Bagging, Decision Tree J48, Support Vector Machine (SVM) and Naive Bayes. The table \ref{tab_classificação}, taken from \citet {bandari_2012}, compares the performances of these models using accuracy as a metric.
            
            
            \begin{table}[h!]
                \centering
                \caption{Classes dos Artigos de Notícias}
                 \begin{tabular}{c c c} 
                     \hline
                     Class name & Range of tweets & Number of articles  \\
                     \hline
                     A & 1-20 & 7,600 \\
                     B & 20-100 & 1,800 \\
                     C & 100-2400 & 600 \\
                     \hline
                     \multicolumn{3}{l}{\small \textbf{Fonte:}\cite{bandari_2012}}
                \end{tabular}
                
                \label{tab_classes}
            \end{table}
    
            \begin{table}[h!]
                \centering
                \caption{Resultados da Classificação}
                \begin{tabular}{l r} 
                     \hline
                     Método & Acurácia  \\
                     \hline
                     Bagging & 83,96\%  \\
                     J48 Decision Trees & 83,75\% \\
                     SVM & 81,54\% \\
                     Naive Bayes & 77,79\% \\
                     \hline
                      \multicolumn{2}{l}{\small \textbf{Fonte:} \cite{bandari_2012}}
                \end{tabular}
                \label{tab_classificação}
            \end{table}
    
A set of attributes, extracted from news articles' content, allows the prediction of popularity before publication with a high degree of accuracy, approximately 84 \%. Using the same attributes, but with a binary rating, shared or not shared, the model predicts that an article will be shared on Twitter with 66 \% accuracy \cite {bandari_2012}.
            
The discovery of significant textual attributes allows, in addition to forecasting, content optimization. In this sense, a proposal was presented in \citet {pereira_proactive_2015} that suggests an Intelligent Decision Support System (IDSS). This system classifies a news article as popular or not and subsequently provides suggestions for simple modifications that would increase its popularity.
            
The popularity of a candidate article is estimated through the prediction module, so the optimization module suggests changes in the structure and content of the article to maximize the expected popularity. The forecast module uses as inputs: digital media content (images, videos); previous popularity of the news referenced in the article; the average number of shares of keywords and natural language attributes \cite {pereira_proactive_2015}. 
    
The authors of \citet {pereira_proactive_2015} used a database with news articles from the website Mashable \footnote {www.mashable.com} covering the two years. The metric for measuring popularity was the number of shares, and they considered a binary classification (popular/unpopular). To obtain a balanced distribution, in the popular class, the articles had more than 1400 shares.
    
The Dirichlet Latent Allocation (LDA) algorithm \cite {blei_latent_nodate} was applied to all texts identifying the five most relevant topics among them. After that, for each article to have the expected popularity, the proximity to these topics was measured using the results as input attributes \cite {pereira_proactive_2015}.
            
Regarding the subjectivity and polarity of the analysis of feelings, the authors adapted and used the Pattern \footnote {http://www.clips.ua.ac.be/pattern} module developed in \cite {smedt_creative_nodate}. Several attributes were extracted to be used, such as subjectivity of the title, subjectivity of the text, polarity of the title, rate of positive and negative words, the polarity of the text, polarity of words, rate of positive words between those that are not neutral and the rate of negative words among those that are not neutral.
    
\citet {pereira_proactive_2015} tested five classification models: Random Forest (RF); Adaptive Boosting (AdaBoost); SVM with a Radial Base Function (RBF); neighboring algorithm (KNN) and Naive Bayes (NB). The following metrics were computed: Accuracy, Precision, Recall, $ F_ {1} $ Score and the Area on the ROC curve (AUC). The results are presented in the table \ref{tab:resultados_4}, taken from \citet {pereira_proactive_2015}.
    
            \begin{table}[H]
                \centering
                \caption{Comparação entre modelos}
                \begin{tabular}{l|c c c c c }
                    \hline
                     Modelo & Acurácia & Precisão & Recall & F1 & AUC  \\
                     \hline
                     Random Forest (RF) & \textbf{0.67} & 0.67 & \textbf{0.71} & \textbf{0.69} & \textbf{0.73} \\
                     Adaptive Boosting (AdaBoost) & 0.66 & 0.68 & 0.67 & 0.67 & 0.72 \\
                     Support Vector Machine (SVM) & 0.66 & 0.67 & 0.68 & 0.68 & 0.71 \\
                     k-Nearest Neighbors (KNN) & 0.62 & 0.66 & 0.55 & 0.60 & 0.67 \\
                     Naive Bayes (NB) & 0.62 & \textbf{0.68} & 0.49 & 0.57 & 0.65 \\
                     \hline 
                     \multicolumn{6}{l}{\small \textbf{Fonte:} \cite{pereira_proactive_2015}}
                \end{tabular}
                \label{tab:resultados_4}
            \end{table}
    
\citet {pereira_proactive_2015} identifies that among the 47 attributes used, those related to keywords, proximity to LDA topics, and article category are among the most important. Applying the optimization to 1,000 articles, the proposed IDSS achieved, on average, a 15 \% increase in popularity. Despite this, the classification methods presented did not obtain good results, but natural language processing techniques to extract attributes from the content proved to be successful.
    
An exciting approach to the issue of attributes is presented in \citet {liu_predicting_2017}. The authors hypothesized that the title's grammatical construction and the abstract could arouse curiosity and attract readers' attention. Attributes obtained with the analysis of feelings and polarity were used; they also included superficial attributes in the predictive models. A new attribute, called \textit{Gramatical Score}, has been proposed to reflect the title's ability to attract users' attention.
    
The jieba tool \footnote {https://pypi.org/project/jieba/}, an open-source tool for word segmentation and word markup, was used. The process to define the \textit {Grammatical Score} followed the following steps:
            \begin{itemize}
                \item Each sentence was divided into words separated by spaces;
                 \item Each word received a grammatical label;
                 \item The quantity of each word was counted in all items.
            \end{itemize}
    
Finally, a table with words, labels, and the number of words was obtained. After that, each item receives a score with the following formula:
    
                \begin{flushright}
                    \begin{equation} \label{GC}
                        gc_i = \sum_{k=0}^n weight(k) \times count(k)
                    \end{equation}
                \end{flushright}
    
In the  equation \ref{GC}, $ gc_i $ represents the Grammatical Score of the $ i $ th item in the dataset and $ k $ represents the $ k $ th word in the $ i $ th item. The $ n $ is the number of words in the title or summary. The $ weight $ is the amount of the $ k $ th word in all news articles, and $ count $ in this equation is the amount of the $ k $ th word in the $ i $ th item.
    
In addition to this attribute, the authors used a logarithmic transformation and normalization by building two new attributes: \textit {categoryscore} and \textit {authorscore}.
                \begin{flushright}
                    \begin{equation} \label{CS}
                        categoryscore = \frac{\sum_nln(s_c)}{n}
                    \end{equation}
                \end{flushright}
    
The $ categoryscore $ is the average view for each category. $ n $ in the equation \ref{CS} represents the total number of news articles by each author. For each category, the data that belonged to this category were selected, and the equation \ref {CS} was used.
    
                \begin{flushright}
                    \begin{equation} \label{AS}
                        authorscore = \frac{\sum_mln(s_a)}{m}
                    \end{equation}
                \end{flushright}
    
The $ authorscore $ is defined in the equation \ref{AS}. $ m $ in the equation represents the total number of news articles by each author. Before calculating the $ authorscore $, data is grouped by author.
            
For the forecast, the authors used the titles and abstracts' length and temporal attributes in addition to the three attributes presented.
    
The authors selected six ranking algorithms to observe the best algorithm for predicting the popularity of news articles. The table \ref{tab:comparacao}, taken from \citet {liu_predicting_2017}, shows the performances. We identified that the ADTree algorithm has the best performance.
    
            \begin{table}[H]
                \centering
                \caption{Comparação de Performance dos Algoritmos}
                \begin{tabular}{c|c|c|c|c|c}
                     \hline
                     Approach & RF & J48 & ADT & NB & BN  \\
                     \hline
                     AUC & 0.825 & 0.743 & 0.837 & 0.77 & 0.825 \\
                     Kappa & 0.504 & 0.472 & 0.523 & 0.407 & 0.493 \\
                     \hline
                      \multicolumn{6}{l}{\small \textbf{Fonte:}\cite{liu_predicting_2017}}
                \end{tabular}
                
                \label{tab:comparacao}
            \end{table}
            
After the study was carried out in \citet {pereira_proactive_2015}, the database was made available in the UCI Machine Learning repository \footnote{https://archive.ics.uci.edu/ml/datasets.php} allowing for new research and experiments. In 2018, \citet {khan_news_2018} presented a new methodology to improve the results presented in \citet {pereira_proactive_2015}.
            
The first analysis was to reduce features to two dimensions using Principal Component Analysis (PCA). PCA is a statistical procedure that uses orthogonal transformations to convert a set of correlated variables into a set of linearly uncorrelated values called principal components. Thus, the two-dimensional PCA analysis output would be two linearly separated sets, but the results of that data set did not allow this separation. Three-dimensional PCA analysis was applied to attempt linear separation and was also unsuccessful \cite {khan_news_2018}.
            
Based on the observation that the features could not be separated linearly and on the trend observed in other studies, the authors sought to test models of non-linear classifiers and methods that use models together such as: Random Forest, Gradient Boosting, AdaBoost, and Bagging. In addition to these, other linear models were tested to prove the effectiveness of the hypothesis.
            
Besides, Recursive Attribute Elimination (RFE) was applied to obtain the 30 main attributes for the classification models. RFE recursively removes the attributes one by one, building a model with the remaining attributes. It continues until a sharp drop in model accuracy is found \cite {khan_news_2018}.
            
The classification task adopted two classes: popular articles with more than 3395 shares, and non-popular. Eleven classification algorithms were applied, showing that the methods that use models together obtained the best results, with Gradient Boosting having the best average accuracy.
            
The Naive Bayes model was the fastest, but it did not perform well because the attributes are not independent. The Perceptron model had its performance deteriorated as the training data increased, which can be explained by the data's non-linearity. So much so that the performance of the perceptron multilayer classifier significantly improved the accuracy of the forecast.
            
Gradient Boosting is a set of models that trains several ``weak'' models and combines them into a `` strong '' model using the gradient optimization. It showed an accuracy of 79 \%, improving the result found in \citet {pereira_proactive_2015}.

    
\subsection{Visual Features}
    
Most studies use the textual attributes and meta-attributes provided by the sites. However, in recent years, with technological advances, it has become possible to use visual attributes extracted directly from videos. One of the first studies in this regard was \citet {trzcinski_recurrent_2017}. The authors studied the problem of predicting the popularity of videos shared on social networks. The forecast was treated as a classification task, and the attributes for the entry of the predictive model were extracted directly from the videos using the Deep Neural Network Architecture.
            
The authors postulated that if the predictive model incorporated the sequentiality of the information presented in the videos, a better classification accuracy would be obtained. In \citet {trzcinski_recurrent_2017}, a method was proposed based on a deep neural network architecture called the Long Term Recurrent Convolutional Network (LRCN), storing sequentiality information as parameters. They called this method \textit{Popularity-LRCN} and evaluated it with a data set of 37,000 videos collected from Facebook \footnote{www.facebook.com}.
            
The Recurring Neural Network Architecture used in \textit {Popularity-LRCN} is a combination of convolutional networks with Long Term Memory units. Inspired by the architecture proposed in \citet{donahue_teste} for the task of recognizing activities in videos. The network entries were sets of 18 frames with sizes of 227 x 227 x 3 for each video. The network consisted of eight layers with learning parameters. The first five layers were convolutional filters, followed by a completely connected layer with 4,096 neurons, LSTM, and the last layer completely connected with two neurons. They used \textit {soft-max} in the classification layer \cite {trzcinski_recurrent_2017}.

To increase the network invariance, layers of \textit {max pooling} were used after the first, second, and fifth convolutional layers. ReLU was used as a nonlinear activation function applied to all convolutional layers' outputs and the layers completely connected. During the training, the 320 x 240 x 3 video frames were randomly reduced to 227 x 227 x 3. Also, the mirroring technique was used to increase the training data set. The network has been trained over 12 seasons (30,000 iterations each) \cite {trzcinski_recurrent_2017}.
        
The network output is a two-dimensional vector, computed from the video frames' propagation through the described architecture, sequentially, obtaining the average rating probability for each video.
            
Data were collected from videos shared on Facebook from June 1, 2016, to September 31, 2016. Due to the massive difference in the videos' number of views (videos with millions of views and videos watched less than 1,000 times), authors used a logarithmic transformation. Also, in order to reduce the bias introduced by the fact that content producers with a large number of followers attract a large number of views, the authors included in the standardization procedure the number of followers of producers \cite {trzcinski_recurrent_2017}. Thus, the normalized popularity score (NPS) is calculated using the formula \ref{normal}:
        
        \begin{flushright}

            \begin{equation}\label{normal}
                NPS = \log_{2} \left( \frac{viewcount +1}{number \ of  \ publisher's \ followers}\right)
            \end{equation}
        \end{flushright}
        
After normalization, the data set was divided into two classes: popular and non-popular. The normalized popularity median enables a balanced distribution of classes.

        
The authors compared the proposed method with traditional classifiers, logistic regression, and SVM with a radial base function to evaluate the method. As input attributes for the classifiers, they used the following sets:
        
        \begin{itemize}
            \item HOG: a resource descriptor with 8,100 dimensions called Oriented Gradient Histogram;
             \item GIST: a resource descriptor with 960 dimensions;
             \item CaffeNet: a convolutional network with an output vector with 1,000 dimensions;
             \item ResNet: Neural Network with an output vector with 1,000 dimensions.
        \end{itemize}
        
The \textit {Popularity-LRCN} surpassed the other methods reaching 70 \% accuracy, according to the table \ref{tab:resultados}. 
        
        \begin{table}[H]
            \centering
            \caption{Resultados dos Modelos de Predição}
            \begin{tabular}{c c c c}
                \hline
                 Model & Features & Classification accuracy & Spearman correlation \\
                 \hline
                 Regressão Logística & HOG & 0.587 $\pm$ 0.006 & 0.229 $\pm$ 0.014 \\
                 & GIST & 0.609 $\pm$ 0.007 & 0.321 $\pm$ 0.008 \\
                 & CaffeNet & 0.622 $\pm$ 0.007 & 0.340 $\pm$ 0.007 \\
                 & ResNet & 0.645 $\pm$ 0.005 & 0.393 $\pm$ 0.010 \\
                 \hline
                SVM & HOG & 0.616 $\pm$ 0.004 & 0.359 $\pm$ 0.008 \\
                 & GIST & 0.609 $\pm$ 0.006 & 0.294 $\pm$ 0.012 \\
                 & CaffeNet & 0.653 $\pm$ 0.003 & 0.395 $\pm$ 0.007 \\
                 & ResNet & 0.650 $\pm$ 0.007 & 0.387 $\pm$ 0.015 \\
                 \hline 
                 Popularity-LRCN & raw video frames & 0.7 $\pm$ 0.003 & 0.521 $\pm$ 0.009 \\
                 \hline
                 \multicolumn{4}{l}{\small \textbf{Fonte:}\cite{trzcinski_recurrent_2017}}
            \end{tabular}
            \label{tab:resultados}
        \end{table}
            
        
    
\subsection{Meta-Data Features}
    
Under development.

\section{Regression Methods} \label{regressao}

In this section, we present the methods used to perform the numerical popularity forecast. The goal is to quantify the degree of popularity. The applications for this type of forecast are diverse, among them, the proactive allocation of resources. It is customary to indicate the forecast error, the correlation coefficient (Pearson or Spearman), and the determination coefficient ($ R ^ 2 $).
        
\subsection{Textual Features}

Under development.
            
\subsection{{Meta-Data Features} \label{MA_regressao}
           
Apesar de apresentarmos diversos métodos que utilizam diversos atributos preditivos, a previsão de popularidade pode ser realizada utilizando apenas o número de visualizações do conteúdo online. Assim, a previsão é executada após a publicação do conteúdo capturando o número de visualizações em um instante $t_{i}$ para prever a popularidade no instante $t_{r}$, com $t_{i} < t_{r}$. Essa ideia, aparentemente simples, traz bons resultados e foi testada com dois portais de compartilhamento: um de notícias, o  Digg\footnote{www.digg,com},  e outro de vídeos, o Youtube\footnote{www.youtube.com} \cite{szabo2010predicting}. 
            
Com as notícias do Digg é possível prever a popularidade do trigésimo dia utilizando o número de visualizações obtido nas primeiras duas horas, enquanto que para o Youtube é preciso utilizar as visualizações obtidas durante os primeiros 10 dias para conseguir prever a popularidade no trigésimo dia. Essa diferença encontra explicação no fato dos ciclos de vida serem distintos \cite{szabo2010predicting}.
            
As notícias tem ciclo de vida curto atraindo atenção rapidamente, mas tendo o interesse dispersado na mesma velocidade. Esse tipo de conteúdo atinge o seu ápice de popularidade rapidamente. Já os vídeos possuem uma taxa de crescimento em constante evolução, proporcionando um ciclo de vida mais longo. A probabilidade de um vídeo atrair grande atenção na web, mesmo depois que o seu ápice de popularidade já passou, é maior do que os artigos de notícias \cite{szabo2010predicting}.
            
\cite{szabo2010predicting} encontraram uma forte correlação (coeficiente de Pearson acima de 0.9) entre as popularidades transformadas logaritmicamente em dois instantes distintos, ou seja, o conteúdo que recebe muitas visualizações logo no início tende a ter maior número de visualizações no futuro. O tempo de referência $t_{r}$ foi definido como o momento no qual a popularidade será prevista e o tempo de indicação $t_{i}$ é o instante no qual a previsão foi executada. Dessa forma, temos $t_{i}<t_{r}$.
            
A correlação encontrada é descrita por um modelo linear com a equação \ref{correlacao_linear}:
            
                \begin{flushright}
                    \begin{equation} \label{correlacao_linear}
                        ln \ N_{c}(t_{2}) = ln \ r(t_{1},t_{2})+ln \ N_{c}(t_{1})+\varepsilon_{c}(t_{1},t_{2})
                    \end{equation}
                \end{flushright}
            
$N_{c}(t)$ é a popularidade do item $c$ desde a publicação até o tempo $t$ e $t_{1}$ e $t_{2}$ são dois instantes escolhidos arbitrariamente, com $t_{2}>t_{1}$. $r(t_{1},t_{2})$ é a relação linear encontrada entre  as transformadas logarítmicas das popularidades  e é independente de $c$. $\varepsilon_{c}$ é o termo de ruído que descreve a aleatoriedade observada nos dados \cite{szabo2010predicting}.
            
\cite{szabo2010predicting} apresentam três modelos preditivos com as  funções de erro a serem minimizadas utilizando a análise de regressão. O primeiro modelo utiliza a regressão linear aplicada em escala logarítmica, a função a ser minimizada é o erro estimado dos mínimos quadrados (LSE) apresentada na equação \ref{LSE}.  $\hat{N}_{c}(t_{i},t_{r})$ é a previsão de popularidade do item $c$ para o instante $t_{r}$ realizada no instante $t_{i}$ e $N_{c}(t_{r})$ é a popularidade real no instante $t_{r}$.
            
                 \begin{flushright}
                    \begin{equation} \label{LSE}
                        LSE = \sum_{c}\left[\hat{N}_{c}(t_{i},t_{r})-N_{c}(t_{r})\right]^2
                    \end{equation}
                \end{flushright}
            
O modelo de regressão que minimiza esta função com dados transformados logaritmicamente é apresentado na equação \ref{LN_previsao}. Nesta equação, $\beta_{0}(t_{i})$ é o coeficiente de regressão e $\tau_{0}^2$ é a variação residual na escala logarítmica.
            
                \begin{flushright}
                    \begin{equation} \label{LN_previsao}
                        \hat{N}_{c}(t_{i},t_{r}) = exp[ln \ N_{c}(t_{i})+\beta_{0}(t_{i})+\tau_{0}^2/2]
                    \end{equation}
                \end{flushright}
            
O segundo modelo utiliza como pressuposto que a evolução de popularidade obedeça a uma escala constante de crescimento. A função de erro a ser minimizada é o erro quadrático relativo (RSE) e está apresentada na equação \ref{RSE}:
            
                \begin{flushright}
                    \begin{equation} \label{RSE}
                       RSE = \sum_{c} \left [\frac{\hat{N}_{c}(t_{i},t_{r})-N_{c}(t_{r})}{N_{c}(t_{r})}\right ]^2
                     \end{equation}
                \end{flushright}
                 
A correspondência linear encontrada entre as popularidades nos tempos iniciais e tempos futuros sugere que o valor da popularidade esperada, $\hat{N}_{c}(t_{i},t_{r})$, para o item  $c$ pode ser expressa como:
            
                 \begin{flushright}
                    \begin{equation} \label{CS_MODELO}
                       \hat{N}_{c}(t_{i},t_{r})=\alpha(t_{i},t_{r})N_{c}(t_{i})
                     \end{equation}
                \end{flushright}
                
$\alpha (t_{i},t_{r}) $ é independente do item $c$, mas dependerá diretamente da função de erro que se deseja minimizar. Neste caso específico, para minimizar RSE teremos:
            
                \begin{flushright}
                    \begin{equation} \label{RSE_CS_MODELO}
                       \alpha(t_{i},t_{r})=\frac{\sum_{c}\frac{N_{c}(t_{i})}{N_{c}(t_{r})}}{\sum_{c}\left [ \frac{N_{c}(t_{i})}{N_{c}(t_{r})}\right ]^2}
                     \end{equation}
                \end{flushright}
                
O terceiro modelo preditivo está baseado na média do perfil de crescimento das popularidades do conjunto de treinamento. Esse perfil de crescimento é calculado como a média das popularidades relativas das submissões no instante $t_{i}$ normalizada pelas popularidades no instante $t_{r}$, assim:
            
                \begin{flushright}
                    \begin{equation} \label{GP_PROFILE}
                       P(t_{i},t_{r})=\left \langle \frac{N_{c}(t_{i})}{N_{c}(t_{r})} \right \rangle_{c}
                     \end{equation}
                \end{flushright}
                
Na equação \ref{GP_PROFILE} $\langle . \rangle_{c}$ é a média das popularidades normalizadas sobre todo o conjunto de treinamento. A previsão de um item  $c$ é calculada com a fórmula \ref{GP_MODELO}:
            
                \begin{flushright}
                    \begin{equation} \label{GP_MODELO}
                       \hat{N}_{c}(t_{r})=\frac{N_{c}(t_i)}{P(t_{i},t_{r})}
                     \end{equation}
                \end{flushright}
        
            
Os modelos apresentados por \cite{szabo2010predicting} são simples e eficientes. Eles comprovam ser possível prever a popularidade futura baseando-se apenas no número de visualizações iniciais, mas eles possuem algumas falhas. Os modelos utilizam como entrada o  número total de visualizações até o dia $t_{i}$, mas dois itens podem apresentar número de visualizações similares em $t_{i}$ e números bem distintos de popularidades em $t_{r}$. Assim, \cite{pinto_using_2013} apresentam dois modelos preditivos que tentam corrigir essas falhas e superam os modelos apresentados em \cite{szabo2010predicting}. 
            
No lugar de utilizar o número total de visualizações obtidas em $t_{i}$, essas visualizações são divididas em  intervalos regulares de medição desde a publicação até o instante $t_{i}$, cada intervalo é denominado de popularidade delta. \cite{pinto_using_2013} propõe um modelo Linear Multivariado  (ML) que prevê a popularidade no instante $t_{r}$ como uma função linear mostrada na equação \ref{ML_MODELO}:
            
                \begin{flushright}
                    \begin{equation} \label{ML_MODELO}
                       \hat{N}_{c}(t_{r})=\Theta(t_{i},t_{r})X_{c}(t_{i})
                     \end{equation}
                \end{flushright}
            
Seja $x_{i}(c)$ o número de visualizações recebidas no intervalo de tempo $\delta_{i}$ e $X_{c}(t_{i})$ o vetor das popularidades para todos os intervalos até $t_{i}$, assim temos a seguinte representação: $X_{c}(t_{i}) = [x_{1}(c), x_{2}(c),x_{3}(c),...,x_{i}(c)]^T$. Os parâmetros do modelo, $\Theta(t_{i},t_{r}) = [\theta_{1},\theta_{2},...,\theta_{i}]$ são computados para minimizar a média do erro quadrático relativo (MRSE), equação \ref{MRSE}.
            
                \begin{flushright}
                    \begin{equation}\label{MRSE}
                        MRSE = \frac{1}{|C|}\sum_{c}\left[ \frac{\hat{N}_{c}(t_{i},t_{r}) - N_{c}(t_{r})}{N_{c}(t_{r})}\right]^2
                    \end{equation}
                \end{flushright}
            
A ideia é que, devido aos diferentes pesos atribuídos aos intervalos de tempo observados no histórico dos itens, o modelo ML consiga capturar o padrão de evolução da popularidade do conteúdo. Entretanto, este modelo ainda é limitado, principalmente para o caso de vídeos que apresentam diferentes padrões de evolução de popularidade. Uma possível solução seria criar um modelo especializado para cada padrão conhecido, porém a grande dificuldade é como saber, a priori, qual será o padrão de evolução do vídeo a ser previsto \cite{pinto_using_2013}
            
Assim, \cite{pinto_using_2013} optaram por construir um modelo que leva em conta a similaridade (em termos de número de visualizações, até $t_{r}$) entre o vídeo e exemplos conhecidos do conjunto de treinamento. Esta similaridade é utilizada para adaptar a previsão de popularidade. Para medir a similaridade entre os vídeos foi utilizada uma função base radial (RBF)  que depende das distâncias da entrada até o centro. Dado o conjunto de treinamento $v_{c}$, o parâmetro $\tau$, o vídeo $v$ e o modelo ML $X(v)$ os autores criaram uma RBF gaussiana apresentada na equação \ref{RBF_function}:
                
                \begin{flushright}
                    \begin{equation}\label{RBF_function}
                        RBF_{v_{c}}(v)=e^{\left(- \frac{||X(v)-X(v_{c})||^2}{2.\tau^2} \right)}
                    \end{equation}
                \end{flushright}
            
No experimento foi selecionada uma amostra aleatória do conjunto de treinamento para ser utilizada como centro da função \ref{RBF_function}. Para cada vídeo $v$ eles computaram $RBF_{v_{c}}(v)$, $C$ é a amostra utilizada como centro, $w_{v_{c}}$ é o peso do modelo associado a RBF \textit{feature}, este modelo foi chamado de MRBF e é formalmente definido pela equação \ref{MRBF_MODELO} \cite{pinto_using_2013}: 
            
                \begin{flushright}
                    \begin{equation}\label{MRBF_MODELO}
                       \hat{N}(v,t_{i},t_{r})=\underbrace{\Theta_{(t_{i},t_{r})}.X(v)}_{modelo \ ML} + \underbrace{\sum_{v_{c} \in C}w_{v_{c}}.RBF_{v_{c}}(v)}_{RBF \ features}
                    \end{equation}
                \end{flushright}
                
Por fim, em \cite{pinto_using_2013} os modelos apresentados são comparados com o modelo de crescimento constante de \cite{szabo2010predicting} chamado de modelo S-H, equação \ref{CS_MODELO}. Os modelos foram comparados aplicando-os em um conjunto de dados de vídeos do YouTube, a métrica de erro utilizada foi o MRSE, os tempos de indicação e referência para os modelos foram: $t_{i}=7$ e $t_{r}=30$ . O Modelo MRBF, como esperado, é o que obtém a melhor performance e os resultados são apresentados na tabela \ref{tab:comparacao_modelos}.
        
        \begin{table}[h!]
                \centering
                \caption{Comparação dos Modelos de Regressão apresentados em \cite{pinto_using_2013}}
                 \begin{tabular}{c c} 
                     \hline
                     Modelo & MRSE  \\
                     \hline
                     Modelo S-H & 0.2121 \pm 0.0074 \\
                     Modelo ML & 0.1837 \pm 0.0081 \\
                     Modelo MRBF & 0.1723 \pm 0.0071 \\
                     \hline
                     \multicolumn{3}{l}{\small \textbf{Fonte:}\cite{pinto_using_2013}}
                \end{tabular}
                \label{tab:comparacao_modelos}
            \end{table}
        \subsection{Atributos Visuais}
        
\cite{trzcinski_predicting_2017} propõem um método de regressão para prever popularidade de vídeos online utilizando Máquina de Vetor Suporte (SVM do inglês \textit{support vector machines}) com função base radial Gaussiana, chamado de \textit{Popularity-SVR}. Este método, quando comparado com os modelos apresentados em \cite{szabo2010predicting} e \cite{pinto_using_2013}, é mais preciso e estável, possivelmente devido ao caráter não linear do \textit{Popularity-SVR}. Nos experimentos de comparação foram utilizados dois conjuntos de dados, com quase 24.000 vídeos, retirados do YouTube\footnote{www.youtube.com} e Facebook\footnote{www.facebook.com}.

Além disso, esse trabalho mostra que a utilização de atributos visuais , como as saídas de redes neurais profundas ou métricas de dinâmica das cenas podem ser úteis para a previsão de popularidade, inclusive por serem obtidos antes da publicação. A acurácia da previsão pode ser melhorada combinando padrões iniciais de distribuição, como nos modelos de \cite{szabo2010predicting} e \cite{pinto_using_2013}, com atributos visuais e sociais como o número de faces que aparecem no vídeo e o número de comentários recebidos pelo vídeo.
            
Os atributos visuais utilizados foram:
            
            \begin{itemize}
                \item \textbf{Características do vídeos.} Foram utilizadas características simples, tais como: comprimento do vídeo, número de frames, resolução do vídeo e as dimensões dos frames.
                \item \textbf{Cor.} Os autores agruparam as cores em 10 classes dependendo das suas coordenadas na representação HSV(matiz,saturação,valor): preto, branco, azul, ciano, verde, amarelo, laranja, vermelho, magenta e outro. Para cada frame foi descoberta a cor predominante, classificando-o em uma dessas 10 classes.
                \item \textbf{Face.} Utilizando um detector de faces foi contabilizado o número de faces por frame, número de frames com faces e o tamanho da região com faces em relação ao tamanho do frame.
                \item \textbf{Texto.} Combinando \textit{Edge Detection} (técnica de processamento de imagem para determinar pontos onde a intensidade luminosa muda repentinamente) e filtros morfológicos foram identificadas regiões do vídeo com textos impressos  gerando os seguintes atributos: quantidade de frames com texto impresso e média do tamanho da região com texto em relação ao tamanho do frame.
                \item \textbf{Dinâmica da Cena.} Utilizando o algoritmo \textit{Edge Change Ration} os autores determinaram os limites dos \textit{shots} (série de imagens consecutivas representando uma ação contínua).O número de \textit{shots} e a média do tamanho dos \textit{shots}, em segundos, foram utilizdos como atributos \cite{jacobs_automatic_nodate}.
                \item \textbf{\textit{Clutter.}}Esta medida representa a desordem do vídeo, os autores utilizaram o \textit{Canny edge detector} para quantificar o \textit{clutter} \cite{canny_computational_nodate}. O atributo utilizado foi a média da proporção dos pixels detectados e a quantidade de pixels em um frame.
                \item \textbf{Rigidez.} Para estimar a rigidez da cena os autores estimaram a homografia entre dois frames consecutivos combinando o uso de FAST \cite{rosten_faster_2010} e BRIEF \cite{calonder_brief_2012}. O atributo era a média do número de homografias válidas encontradas.
                \item \textbf{\textit{Thumbnail}.} Foi computado a popularidade para a \textit{thumbnail} do vídeo utilizando a \textit{Popularity API}\footnote{Disponível em: http://popularity.csail.mit.edu/} seguindo o trabalho de \cite{khosla_what_2014}.
                \item \textbf{\textit{Deep Features.}} Foi utilizada uma rede neural convolucional de 152 camadas chamada ResNet-152 \cite{he_deep_2015}. Para cada vídeo foi extraído um conjunto de \textit{thumbnails} por cena propagando-as pela ResNet-152.  A saída obtida era um vetor de 1000 dimensões. Este vetor foi normalizado resultando em um único valor.
            \end{itemize}
                
Além dos atributos visuais acima, foram utilizadas como variáveis preditivas o número de visualizações e as características sociais como quantidade de compartilhamentos, \textit{likes} e comentários. Os métodos preditivos utilizados para comparação são os apresentados em \cite{szabo2010predicting} e \cite{pinto_using_2013} e explicados na seção \ref{MA_regressao}. 
            
O modelo de regressão MRBF, explicitado pela equação \ref{MRBF_MODELO}, apresenta a combinação de dois métodos: o modelo de regressão ML (linear) e RBF \textit{features} (não-linear). Não é necessário realizar esta previsão em duas etapas, inspirado pelos resultados do MRBF, o \textit{Popularity-SVR} utiliza função base radial Gaussiana (RBF) como o kernel da transformação, permitindo mapear o vetor de atributos em um espaço não-linear onde as relações dos padrões de evolução dos vídeos são mais fáceis de capturar \cite{trzcinski_predicting_2017}.
            
As máquinas de vetores de suporte criam superfícies de separação para conjunto de dados  linearmente separáveis ou que possuam uma distribuição aproximadamente linear. Porém, em problemas não lineares isso não é possível. Pode-se conseguir essa separação linear mapeando o conjunto de entradas do espaço original para um espaço de dimensão superior.  \cite{faceli_inteligencia_2011}. 
            
Seja $\Phi : X \rightarrow \Im$ um mapeamento, em que $X$ é o espaço de entradas e $\Im$ denota o espaço de características. A escolha apropriada de $\Phi$ faz com que o conjunto de treinamento mapeado em $\Im$ possa ser separado por uma SVM linear. \cite{faceli_inteligencia_2011}
            
Um kernel $K$ é uma função que recebe dois pontos $x_{i}$ e $x_{j}$ no espaço de entradas e calcula o produto escalar desses objetos no espaço de características, mapeando o conjunto de entrada em um novo espaço dimensional. \cite{faceli_inteligencia_2011}
            
Como resultado, o caráter não linear do kernel RBF de transformação permite uma predição robusta baseando-se na similaridade com os padrões de evolução de popularidade  identificados  no conjunto de treinamento. Essa proposta difere do modelo MRBF que realiza a comparação de similaridade com um conjunto de vídeos selecionados aleatoriamente do conjunto de treinamento \cite{trzcinski_predicting_2017}. 
            
A seleção do kernel correto pode influenciar a performance do modelo, por esta razão busca-se um kernel ótimo. A popularidade de um vídeo $v$ utilizando o método \textit{Popularity-SVR} pode ser calculado  como na equação \ref{PSVR_MODELO} \cite{trzcinski_predicting_2017}.
            
            \begin{flushright}
                    \begin{equation} \label{PSVR_MODELO}
                        \hat{N}(v,t_{i},t_{r})= \sum_{k=1}^{K}\alpha_{k}.\Phi \left ( X(v,t_{i}),X(k,t_{i})\right)+b
                    \end{equation}
                \end{flushright}
            
            
Na equação \ref{PSVR_MODELO},  $\Phi (x,y)=exp\left(-\frac{||x-y||^{2}}{\sigma ^{2}}\right)$ é uma RBF gaussiana com parâmetro $\sigma$, $X(v,t_{i})$ é o vetor de atributos para o vídeo $v$ disponível no instante $t_{i}$ e $\left { X(k,t_{i}) \right }_{k=1}^{K} $ é o conjunto de vetores suporte retornados pelo algoritmo SVR com o conjunto de coeficientes $\left { \alpha_{k} \right }_{k=1}^{K}$ e intercepta $b$. Os autores encontraram valores ótimos para o hiper-parâmetro $C$ da otimização  Máquina de Vetor Suporte e  $\sigma $ para o RBF kernel utilizando um grid de busca com o método Python scikit's \textsl{sklearn.grid\_search.GridSearchCV} em um conjunto preliminar de experimentos. Os valores encontrados foram: $C=10$ e $\sigma=0.005$ \cite{trzcinski_predicting_2017}.
            
O \textit{Popularity-SVR} foi comparado com outros modelos de regressão utilizando-se dois conjuntos de dados. O primeiro conjunto de dados eram vídeos do YouTube e o segundo conjunto de dados, também de vídeos, foi extraído de diferentes perfis do Facebook. Primeiramente, o \textit{Popularity-SVR} foi comparado com o modelo de predição apresentado em \cite{szabo2010predicting}, que chamaremos de modelo S-H, e os modelos ML e MRBF apresentados em \cite{pinto_using_2013} utilizando  o número de visualizações dos vídeos do YouTube com $t_{i}=6 \ dias$ e $t_{r}=30 \ dias$. A métrica utilizada para comparação foi o coeficiente de correlação de Spearman e os melhores resultados são apresentados na tabela \ref{tab:comparacao_PSVR}.
            
            \begin{table}[h!]
                \centering
                \caption{Comparação dos Modelos com dados do YouTube}
                 \begin{tabular}{l r} 
                     \hline
                     Modelo & Spearman  \\
                     \hline
                     Modelo S-H & 0.8797 \pm 0.0018 \\
                     Modelo ML & 0.8921 \pm 0.017 \\
                     Modelo MRBF & 0.9046 \pm 0.0152 \\
                     \textit{Popularity-SVR} & 0.9353 \pm 0.009 \\
                      \hline
                      \multicolumn{3}{l}{\small \textbf{Fonte:}\cite{trzcinski_predicting_2017}}
                \end{tabular}
                \label{tab:comparacao_PSVR}
            \end{table}
            
A outra comparação utilizou o conjunto de dados do Facebook, testando os modelos apenas com o número de visualizações, posteriormente somente com os dados sociais, somente com os atributos visuais e combinando os atributos sociais, visuais e número de visualizações. A predição com as informações visuais obteve o pior desempenho, mas quando todos os atributos são combinados a predição é mais precisa comprovando a vantagem de se utilizar todos os conjuntos de atributos de forma combinada. Os resultados estão apresentados na tabela \ref{tab:comparacao_PSVR_facebook}.
            
            \begin{table}[h!]
                \centering
                \caption{Comparação dos Modelos com dados do Facebook}
                 \begin{tabular}{l r} 
                     \hline
                     Modelo & Spearman  \\
                     \hline
                     Modelo S-H & 0.9148 \pm 0.0032 \\
                     Modelo ML & 0.925 \pm 0.0032 \\
                     Modelo MRBF & 0.9203 \pm 0.0366 \\
                     \textit{Popularity-SVR} & 0.9413 \pm 0.0127 \\
                      \hline
                      \multicolumn{3}{l}{\small \textbf{Fonte:}\cite{trzcinski_predicting_2017}}
                \end{tabular}
                \label{tab:comparacao_PSVR_facebook}
            \end{table}
            
O método \textit{Popularity-SVR} proposto em \cite{trzcinski_predicting_2017} é uma evolução dos métodos apresentados em \cite{szabo2010predicting} e \cite{pinto_using_2013}, superando-os em performance. Além disso, a utilização de  um conjunto de atributos visuais combinados com o número de visualizações e dados sociais dos vídeos incrementa a performance do previsor de popularidade. Essas informações podem ser extraídas dos vídeos antes da publicação podendo ser utilizadas em outros modelos de previsão.

\section{Conclusão} \label{conclusao}

        Este trabalho apresenta o estado da arte da previsão de popularidade de Conteúdo na Internet utilizando Inteligência Artificial. Foi apresentada taxonomia para classificação dos métodos, breve descrição da fundamentação teórica que permite o entendimento das teorias apresentadas, bem como os métodos e resultados obtidos para a previsão de popularidade, tanto para classificação quanto para a regressão.
        
        Foram apresentados métodos e técnicas de Processamento de Linguagem Natural que permitem a extração de características relevantes e os métodos de aprendizado de máquina para a previsão, desde os métodos clássicos até a utilização de redes neurais profundas e métodos agregadores como o \textit{Random Forest}.
        
        Além da apresentação teórica, descrevemos a aplicação de um exemplo prático descrito na linguagem Python utilizando a biblioteca scikit-learn. 
        
        

\newpage
%\section{Bibliografia principal utilizada na preparação do curso } 
   % \bibliographystyle{sbc}
    %\bibliography{sbc-template.bib}
    \printbibliography

\newpage

    \section{Curriculum Vitae resumido dos autores} 
        \begin{small}
    
            \textbf{Sidney Loyola de Sá} possui graduação em Sistemas de Computação pela Universidade Federal Fluminense (2013), especialização em Gestão de Projetos pela Universidade Cândido Mendes (2016)  e mestrando em Ciências da Computação pela Universidade Federal Fluminense (2019). Atualmente é Analista em Ciência e Tecnologia na Comissão Nacional de Energia Nuclear, atuando na área de Gestão e Governança de Tecnologia da Informação contribuindo para elaboração de Políticas e Planos, incluindo o Plano Diretor de Tecnologia da Informação. Gerenciou diversos projetos de implantação tais como Service Desk, Rede Corporativa MPLS, GED/Workflow, Fábrica de Software, dentre outros. Tem experiência na área de Sistemas de Computação, atuando principalmente nos seguintes temas: Segurança da Informação, Sistemas de Informação,  Integração de Sistemas, Aprendizado de Máquina. 
    
            \textbf{Aline Paes} é professora adjunta do Instituto de Computação da Universidade Federal Fluminense (UFF). É mestre e doutora em Engenharia de Sistemas e Computação, com ênfase em Inteligência Artificial, pela COPPE-Sistemas, UFRJ, tendo feito estágio de doutoramento (sanduíche) por um ano no Imperial College London, UK, sob a supervisão do Professor Stephen Muggleton. Foi bolsista do CNPq de pós-doutorado júnior na COPPE-Sistemas, UFRJ, sob a supervisão do professor Valmir Carneiro Barbosa e é Jovem Cientista do Nosso Estado pela FAPERJ. Aline Paes atua na área de Ciência da Computação, com ênfase em Inteligência Artificial, com interesses e contribuições nos seguintes temas: aprendizado de máquina relacional, integrado a técnicas neurais, estatísticas e lógicas, atualização e adaptação de modelos por aprendizado online, revisão de teorias e aprendizado por transferência, IA explicável, indução de programas, processamento de linguagem natural, jogos e IA social. 
    
    
            \textbf{Antonio A de A Rocha} é professor do IC/UFF, com doutorado na COPPE/UFRJ e pós-doutorado pela UMass-Amherst. Foi bolsista de Produtividade em Pesquisa do CNPq (PQ nível 2) de 2013 a 2016. \textbf{Foi autor de três minicursos em Simpósios Brasileiros (SBSeg 2012, SBSeg 2017 e SBRC 2016), sendo que o do SBSeg 2017 tratava do tema de Segurança IoT utilizando Blockchain. Foi ainda Coordenador de Minicursos do SBRC 2015.} Ele tem artigos em importantes conferências e periódicos internacionais, sendo alguns deles premiados, incluindo os de melhores artigos no ACM/CoNEXT 2009 e SBRC 2007. Tem interesse e trabalhos na áreas de Redes de Computadores, Sistemas Distribuídos, Segurança em Sistemas Computacionais, Análise de Desempenho e Ciência de Dados.
        \end{small}


\end{document}